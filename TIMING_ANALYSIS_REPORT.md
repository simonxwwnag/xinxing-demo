# 知识库搜索 API 时间消耗分析报告

## 测试数据
- **产品名称**: "光纤收发器"
- **产品特征**: "1.名称:光纤收发器\n2.规格:详设计"

## API 调用流程

### 完整流程时间线

```
/api/knowledge/search
├── [步骤1] search_specs() - 规格搜索
│   ├── 知识库API调用 (search_knowledge)
│   │   ├── 连接超时: 60秒
│   │   ├── 读取超时: 120秒
│   │   └── 返回结果: 最多4条
│   └── 预计耗时: 5-30秒
│
└── [步骤2] search_suppliers_from_docs() - 供应商搜索
    ├── [2.1] 集团定商采购文档搜索
    │   ├── 知识库API调用
    │   │   ├── limit=30 (返回30个结果)
    │   │   └── 预计耗时: 5-15秒
    │   │
    │   └── 对每个structured chunk调用LLM
    │       ├── 调用次数: 最多30次（串行）
    │       ├── 每次超时: 180秒
    │       ├── 实际耗时: 5-30秒/次
    │       └── 累计耗时: 150-900秒（2.5-15分钟）
    │
    └── [2.2] 油田定商采购文档搜索
        ├── 知识库API调用
        │   ├── limit=30 (返回30个结果)
        │   └── 预计耗时: 5-15秒
        │
        └── 对每个structured chunk调用LLM
            ├── 调用次数: 最多30次（串行）
            ├── 每次超时: 180秒
            ├── 实际耗时: 5-30秒/次
            └── 累计耗时: 150-900秒（2.5-15分钟）
```

## 时间消耗分析

### 1. 规格搜索 (search_specs)

**代码位置**: `backend/services/knowledge_service.py:217-561`

**耗时组成**:
- 知识库API调用: 5-30秒
- 响应解析: <1秒
- **总耗时**: 5-30秒

**优化空间**: 较小，已经限制返回4条结果

### 2. 供应商搜索 (search_suppliers_from_docs)

**代码位置**: `backend/services/knowledge_service.py:563-612`

**耗时组成**:

#### 2.1 知识库API调用
- **集团文档**: 5-15秒
- **油田文档**: 5-15秒
- **小计**: 10-30秒

#### 2.2 LLM调用（主要瓶颈）

**代码位置**: `backend/services/knowledge_service.py:753-998`

**关键问题**:
```python
# 每个文档返回30个结果
"limit": 30,  # backend/services/knowledge_service.py:660

# 对每个structured chunk串行调用LLM
if chunk_type == 'structured':
    supplier = self._extract_supplier_from_structured(...)  # 串行调用
    # backend/services/knowledge_service.py:697-705

# LLM调用超时设置
timeout=180.0  # backend/services/knowledge_service.py:912
```

**时间计算**:

| 场景 | LLM调用次数 | 每次耗时 | 累计耗时 |
|------|------------|---------|---------|
| 理想情况（每个5秒） | 60次（30×2） | 5秒 | **300秒（5分钟）** |
| 一般情况（每个10秒） | 60次（30×2） | 10秒 | **600秒（10分钟）** |
| 最坏情况（每个30秒） | 60次（30×2） | 30秒 | **1800秒（30分钟）** |

**实际估算**:
- 如果返回30个structured chunks
- 每个LLM调用平均10秒
- 两个文档: 30 × 10 × 2 = **600秒（10分钟）**
- **远超前端120秒超时限制**

### 3. 总耗时估算

| 步骤 | 最小耗时 | 一般耗时 | 最大耗时 |
|------|---------|---------|---------|
| 规格搜索 | 5秒 | 15秒 | 30秒 |
| 供应商搜索 | 160秒 | 610秒 | 1830秒 |
| **总计** | **165秒** | **625秒** | **1860秒** |

**结论**: 
- ✅ 最小情况: 165秒（超过120秒超时）
- ❌ 一般情况: 625秒（10分钟，远超超时）
- ❌ 最坏情况: 1860秒（31分钟，严重超时）

## 性能瓶颈定位

### 🔴 主要瓶颈: LLM串行调用

**问题代码**:
```697:705:backend/services/knowledge_service.py
if chunk_type == 'structured':
    # 传递产品信息，让LLM在提取时同时判断相关性
    supplier = self._extract_supplier_from_structured(
        point, doc_id, doc_name, point_slice_id, 
        product_name=product_name, 
        product_features=product_features
    )
    if supplier:
        suppliers.append(supplier)
```

**问题分析**:
1. **串行处理**: 每个chunk都要等待前一个LLM调用完成
2. **调用次数多**: 每个文档最多30次，两个文档共60次
3. **超时设置长**: 每次最多180秒，实际可能需要5-30秒
4. **累计时间长**: 60次 × 10秒 = 600秒（10分钟）

### 🟡 次要瓶颈: 知识库API返回结果过多

**问题代码**:
```660:660:backend/services/knowledge_service.py
"limit": 30,  # 返回30个结果，后续通过AI过滤
```

**问题分析**:
- 返回30个结果，但大部分可能不相关
- 需要调用LLM过滤，增加了LLM调用次数

### 🟢 前端超时设置

**问题代码**:
```55:55:frontend/src/services/api.ts
timeout: 120000, // 知识库查询+AI总结可能需要较长时间，设置为120秒
```

**问题分析**:
- 前端超时120秒
- 但后端实际需要600+秒
- 导致前端超时

## 优化方案

### 方案1: 减少返回结果数量（推荐，立即生效）

**修改位置**: `backend/services/knowledge_service.py:660`

```python
# 修改前
"limit": 30,  # 返回30个结果，后续通过AI过滤

# 修改后
"limit": 10,  # 减少返回结果，降低LLM调用次数
```

**效果**:
- LLM调用次数: 60次 → 20次（减少66%）
- 预计耗时: 600秒 → 200秒
- **仍然超过120秒，但大幅改善**

### 方案2: 降低LLM超时时间（推荐）

**修改位置**: `backend/services/knowledge_service.py:912`

```python
# 修改前
timeout=180.0  # 设置180秒超时

# 修改后
timeout=30.0  # 设置30秒超时，避免单个调用耗时过长
```

**效果**:
- 避免单个调用耗时过长
- 快速失败，不阻塞其他调用

### 方案3: 增加前端超时时间（临时方案）

**修改位置**: `frontend/src/services/api.ts:55`

```typescript
// 修改前
timeout: 120000, // 120秒

// 修改后
timeout: 300000, // 300秒（5分钟）
```

**效果**:
- 给后端更多处理时间
- 但用户体验较差（等待时间长）

### 方案4: 组合优化（最佳方案）

**同时实施**:
1. ✅ 减少返回结果: `limit=30 → limit=10`
2. ✅ 降低LLM超时: `timeout=180 → timeout=30`
3. ✅ 增加前端超时: `timeout=120 → timeout=180`（3分钟）

**预计效果**:
- LLM调用次数: 60次 → 20次
- 每次LLM调用: 10秒 → 10秒（不变）
- 总耗时: 600秒 → 200秒
- 前端超时: 120秒 → 180秒
- **✅ 在180秒内完成，避免超时**

## 时间消耗占比分析

### 当前情况（limit=30）

```
总耗时: 625秒（一般情况）
├── 规格搜索: 15秒 (2.4%)
└── 供应商搜索: 610秒 (97.6%)
    ├── 知识库API: 20秒 (3.2%)
    └── LLM调用: 590秒 (94.4%)
        ├── 集团文档: 295秒 (47.2%)
        └── 油田文档: 295秒 (47.2%)
```

### 优化后（limit=10）

```
总耗时: 215秒（一般情况）
├── 规格搜索: 15秒 (7.0%)
└── 供应商搜索: 200秒 (93.0%)
    ├── 知识库API: 20秒 (9.3%)
    └── LLM调用: 180秒 (83.7%)
        ├── 集团文档: 90秒 (41.9%)
        └── 油田文档: 90秒 (41.9%)
```

## 结论

### 🔴 主要问题
1. **LLM串行调用是最大瓶颈**，占总耗时的94.4%
2. **返回结果数量过多**（limit=30），导致LLM调用次数过多
3. **前端超时设置过短**（120秒），无法覆盖实际耗时

### ✅ 推荐解决方案
1. **立即实施**: 减少返回结果数量（limit=10）
2. **立即实施**: 降低LLM超时时间（timeout=30）
3. **立即实施**: 增加前端超时时间（timeout=180）

**预计效果**: 总耗时从625秒降低到215秒，在180秒超时内完成（留有余量）

